{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\n",
    "* **Authors**: Tim Salimans, Diederik P. Kingma\n",
    "* **Link**: http://arxiv.org/abs/1602.07868\n",
    "* **Tags**: Neural Network, Normalization, VAE\n",
    "* **Year**: 2016\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What it is\n",
    "  * Weight Normalization (WN) is a normalization technique, similar to Batch Normalization (BN).\n",
    "  * It normalizes each layer's weights.\n",
    "\n",
    "* Differences to BN\n",
    "  * WN normalizes based on each weight vector's orientation and magnitude. BN normalizes based on each weight's mean and variance in a batch.\n",
    "  * WN works on each example on its own. BN works on whole batches.\n",
    "  * WN is more deterministic than BN (due to not working an batches).\n",
    "  * WN is better suited for noisy environment (RNNs, LSTMs, reinforcement learning, generative models). (Due to being more deterministic.)\n",
    "  * WN is computationally simpler than BN.\n",
    "\n",
    "* How its done\n",
    "  * WN is a module added on top of a linear or convolutional layer.\n",
    "  * If that layer's weights are `w` then WN learns two parameters `g` (scalar) and `v` (vector, identical dimension to `w`) so that `w = gv / ||v||` is fullfilled (`||v||` = euclidean norm of v).\n",
    "  * `g` is the magnitude of the weights, `v` are their orientation.\n",
    "  * `v` is initialized to zero mean and a standard deviation of 0.05.\n",
    "  * For networks without recursions (i.e. not RNN/LSTM/GRU):\n",
    "    * Right after initialization, they feed a single batch through the network.\n",
    "    * For each neuron/weight, they calculate the mean and standard deviation after the WN layer.\n",
    "    * They then adjust the bias to `-mean/stdDev` and `g` to `1/stdDev`.\n",
    "    * That makes the network start with each feature being roughly zero-mean and unit-variance.\n",
    "    * The same method can also be applied to networks without WN.\n",
    "\n",
    "* Results:\n",
    "  * They define BN-MEAN as a variant of BN which only normalizes to zero-mean (not unit-variance).\n",
    "  * CIFAR-10 image classification (no data augmentation, some dropout, some white noise):\n",
    "    * WN, BN, BN-MEAN all learn similarly fast. Network without normalization learns slower, but catches up towards the end.\n",
    "    * BN learns \"more\" per example, but is about 16% slower (time-wise) than WN.\n",
    "    * WN reaches about same test error as no normalization (both ~8.4%), BN achieves better results (~8.0%).\n",
    "    * WN + BN-MEAN achieves best results with 7.31%.\n",
    "    * Optimizer: Adam\n",
    "  * Convolutional VAE on MNIST and CIFAR-10:\n",
    "    * WN learns more per example und plateaus at better values than network without normalization. (BN was not tested.)\n",
    "    * Optimizer: Adamax\n",
    "  * DRAW on MNIST (heavy on LSTMs):\n",
    "    * WN learns significantly more example than network without normalization.\n",
    "    * Also ends up with better results. (Normal network might catch up though if run longer.)\n",
    "  * Deep Reinforcement Learning (Space Invaders):\n",
    "    * WN seemed to overall acquire a bit more reward per epoch than network without normalization. Variance (in acquired reward) however also grew.\n",
    "    * Results not as clear as in DRAW.\n",
    "    * Optimizer: Adamax\n",
    "\n",
    "* Extensions\n",
    "  * They argue that initializing `g` to `exp(cs)` (`c` constant, `s` learned) might be better, but they didn't get better test results with that.\n",
    "  * Due to some gradient effects, `||v||` currently grows monotonically with every weight update. (Not necessarily when using optimizers that use separate learning rates per parameters.)\n",
    "  * That grow effect leads the network to be more robust to different learning rates.\n",
    "  * Setting a small hard limit/constraint for `||v||` can lead to better test set performance (parameter updates are larger, introducing more noise).\n",
    "\n",
    "\n",
    "![CIFAR-10 results](images/Weight_Normalization__cifar10.png?raw=true \"CIFAR-10 results\")\n",
    "\n",
    "*Performance of WN on CIFAR-10 compared to BN, BN-MEAN and no normalization.*\n",
    "\n",
    "![DRAW, DQN results](images/Weight_Normalization__draw_dqn.png?raw=true \"DRAW, DQN results\")\n",
    "\n",
    "*Performance of WN for DRAW (left) and deep reinforcement learning (right).*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
