{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: Character-based Neural Machine Translation\n",
    "* **Authors**: Marta R. Costa-Jussà, José A. R. Fonollosa\n",
    "* **Link**: http://arxiv.org/abs/1603.00810v3\n",
    "* **Tags**: Neural Network, machine translation\n",
    "* **Year**: 2016\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What\n",
    "  * Most neural machine translation models currently operate on word vectors or one hot vectors of words.\n",
    "  * They instead generate the vector of each word on a character-level.\n",
    "    * Thereby, the model can spot character-similarities between words and treat them in a similar way.\n",
    "    * They do that only for the source language, not for the target language.\n",
    "\n",
    "* How\n",
    "  * They treat each word of the source text on its own.\n",
    "  * To each word they then apply the model from [Character-aware neural language models](https://arxiv.org/abs/1508.06615), i.e. they do per word:\n",
    "    * Embed each character into a 620-dimensional space.\n",
    "    * Stack these vectors next to each other, resulting in a 2d-tensor in which each column is one of the vectors (i.e. shape `620xN` for `N` characters).\n",
    "    * Apply convolutions of size `620xW` to that tensor, where a few different values are used for `W` (i.e. some convolutions cover few characters, some cover many characters).\n",
    "    * Apply a tanh after these convolutions.\n",
    "    * Apply a max-over-time to the results of the convolutions, i.e. for each convolution use only the maximum value.\n",
    "    * Reshape to 1d-vector.\n",
    "    * Apply two highway-layers.\n",
    "    * They get 1024-dimensional vectors (one per word).\n",
    "    * Visualization of their steps:\n",
    "      * ![Architecture](images/Character-based_Neural_Machine_Translation__architecture.jpg?raw=true \"Architecture\")\n",
    "  * Afterwards they apply the model from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) to these vectors, yielding a translation to a target language.\n",
    "  * Whenever that translation yields an unknown target-language-word (\"UNK\"), they replace it with the respective (untranslated) word from the source text.\n",
    "\n",
    "* Results\n",
    "  * They the German-English [WMT](http://www.statmt.org/wmt15/translation-task.html) dataset.\n",
    "  * BLEU improvemements (compared to neural translation without character-level words):\n",
    "    * German-English improves by about 1.5 points.\n",
    "    * English-German improves by about 3 points.\n",
    "  * Reduction in the number of unknown target-language-words (same baseline again):\n",
    "    * German-English goes down from about 1500 to about 1250.\n",
    "    * English-German goes down from about 3150 to about 2650.\n",
    "  * Translation examples (Phrase = phrase-based/non-neural translation, NN = non-character-based neural translation, CHAR = theirs):\n",
    "    * ![Examples](images/Character-based_Neural_Machine_Translation__examples.jpg?raw=true \"Examples\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
