{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: Convolutional Pose Machines\n",
    "* **Authors**: Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh\n",
    "* **Link**: https://arxiv.org/abs/1602.00134\n",
    "* **Tags**: Neural Network, pose\n",
    "* **Year**: 2016\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What\n",
    "  * They suggest a new model for human pose estimation (i.e. to lay a \"skeleton\" over the image of a person).\n",
    "  * Their model has a (more or less) recurrent architecture.\n",
    "    * Initial estimates of keypoint locations are refined in several steps.\n",
    "    * The idea of the recurrent architecture is derived from message passing, unrolled into one feed-forward model.\n",
    "\n",
    "* How\n",
    "  * Architecture\n",
    "    * They generate the end result in multiple steps, similar to a recurrent network.\n",
    "    * Step 1:\n",
    "      * Receives the image (368x368 resolution).\n",
    "      * Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n",
    "    * Step 2 and later:\n",
    "      * (Modified) Receives the image (368x368 resolution) and the previous likelihood scores.\n",
    "      * (Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n",
    "      * (New) Concatenates the likelihoods with the likelihoods of the previous step.\n",
    "      * (New) Applies a few more convolutions to the concatenation to compute the final likelihood scores.\n",
    "    * Visualization of the architecture:\n",
    "      * ![Architecture](images/Convolutional_Pose_Machines__architecture.jpg?raw=true \"Architecture\")\n",
    "  * Loss function\n",
    "    * The basic loss function is a simple mean squared error between the expected output maps per keypoint and the predicted ones.\n",
    "    * In the expected output maps they mark the correct positions of the keypoints using a small gaussian function.\n",
    "    * They apply losses after each step in the architecture, argueing that this helps against vanishing gradients (they don't seem to be using BN).\n",
    "    * The expected output maps of the first step actually have the positions of all keypoints of a certain type (e.g. neck) marked, i.e. if there are multiple people in the extracted image patch there might be multiple correct keypoint positions. Only at step 2 and later they reduce that to the expected person (i.e. one keypoint position per map).\n",
    "\n",
    "* Results\n",
    "  * Example results:\n",
    "    * ![Example results](images/Convolutional_Pose_Machines__results.jpg?raw=true \"Example results\")\n",
    "  * Self-correction of predictions over several timesteps:\n",
    "    * ![Effect of timesteps](images/Convolutional_Pose_Machines__timesteps.jpg?raw=true \"Effect of timesteps\")\n",
    "  * They beat existing methods on the datasets MPII, LSP and FLIC.\n",
    "  * Applying a loss function after each step (instead of only once after the last step) improved their results and reduced problems related to vanishing gradients.\n",
    "  * The effective receptive field size of each step had a significant influence on the results. They increased it to up to 300px (about 80% of the image size) and saw continuous improvements in accuracy.\n",
    "    * ![Receptive field size effect](images/Convolutional_Pose_Machines__rf_size.jpg?raw=true \"Receptive field size effect\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
