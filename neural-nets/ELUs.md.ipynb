{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: Fast and Accurate Deep Networks Learning By Exponential Linear Units (ELUs)\n",
    "* **Authors**: Djork-ArnÃ© Clevert, Thomas Unterthiner, Sepp Hochreiter\n",
    "* **Link**: http://arxiv.org/abs/1511.07289\n",
    "* **Tags**: Neural Network, activation, nonlinearity, ReLU, ELU, PReLU, LeakyReLU\n",
    "* **Year**: 2015\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What\n",
    "  * ELUs are an activation function\n",
    "  * The are most similar to LeakyReLUs and PReLUs\n",
    "\n",
    "* How (formula)\n",
    "  * f(x):\n",
    "    * `if x >= 0: x`\n",
    "    * `else: alpha(exp(x)-1)`\n",
    "  * f'(x) / Derivative:\n",
    "    * `if x >= 0: 1`\n",
    "    * `else: f(x) + alpha`\n",
    "  * `alpha` defines at which negative value the ELU saturates.\n",
    "  * E. g. `alpha=1.0` means that the minimum value that the ELU can reach is `-1.0`\n",
    "  * LeakyReLUs however can go to `-Infinity`, ReLUs can't go below 0.\n",
    "\n",
    "![ELUs vs LeakyReLUs vs ReLUs](images/ELUs__slopes.png?raw=true \"ELUs vs LeakyReLUs vs ReLUs\")\n",
    "\n",
    "*Form of ELUs(alpha=1.0) vs LeakyReLUs vs ReLUs.*\n",
    "\n",
    "\n",
    "* Why\n",
    "  * They derive from the unit natural gradient that a network learns faster, if the mean activation of each neuron is close to zero.\n",
    "  * ReLUs can go above 0, but never below. So their mean activation will usually be quite a bit above 0, which should slow down learning.\n",
    "  * ELUs, LeakyReLUs and PReLUs all have negative slopes, so their mean activations should be closer to 0.\n",
    "  * In contrast to LeakyReLUs and PReLUs, ELUs saturate at a negative value (usually -1.0).\n",
    "  * The authors think that is good, because it lets ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence.\n",
    "  * So ELUs can measure the presence of concepts quantitatively, but the absence only qualitatively.\n",
    "  * They think that this makes ELUs more robust to noise.\n",
    "\n",
    "* Results\n",
    "  * In their tests on MNIST, CIFAR-10, CIFAR-100 and ImageNet, ELUs perform (nearly always) better than ReLUs and LeakyReLUs.\n",
    "  * However, they don't test PReLUs at all and use an alpha of 0.1 for LeakyReLUs (even though 0.33 is afaik standard) and don't test LeakyReLUs on ImageNet (only ReLUs).\n",
    "\n",
    "![CIFAR-100](images/ELUs__cifar100.png?raw=true \"CIFAR-100\")\n",
    "\n",
    "*Comparison of ELUs, LeakyReLUs, ReLUs on CIFAR-100. ELUs ends up with best values, beaten during the early epochs by LeakyReLUs. (Learning rates were optimized for ReLUs.)*\n",
    "\n",
    "-------------------------\n",
    "\n",
    "# Rough chapter-wise notes\n",
    "\n",
    "* Introduction\n",
    "  * Currently popular choice: ReLUs\n",
    "  * ReLU: max(0, x)\n",
    "  * ReLUs are sparse and avoid the vanishing gradient problem, because their derivate is 1 when they are active.\n",
    "  * ReLUs have a mean activation larger than zero.\n",
    "  * Non-zero mean activation causes a bias shift in the next layer, especially if multiple of them are correlated.\n",
    "  * The natural gradient (?) corrects for the bias shift by adjusting the weight update.\n",
    "  * Having less bias shift would bring the standard gradient closer to the natural gradient, which would lead to faster learning.\n",
    "  * Suggested solutions:\n",
    "    * Centering activation functions at zero, which would keep the off-diagonal entries of the Fisher information matrix small.\n",
    "    * Batch Normalization\n",
    "    * Projected Natural Gradient Descent (implicitly whitens the activations)\n",
    "  * These solutions have the problem, that they might end up taking away previous learning steps, which would slow down learning unnecessarily.\n",
    "  * Chosing a good activation function would be a better solution.\n",
    "  * Previously, tanh was prefered over sigmoid for that reason (pushed mean towards zero).\n",
    "  * Recent new activation functions:\n",
    "    * LeakyReLUs: x if x > 0, else alpha*x\n",
    "    * PReLUs: Like LeakyReLUs, but alpha is learned\n",
    "    * RReLUs: Slope of part < 0 is sampled randomly\n",
    "  * Such activation functions with non-zero slopes for negative values seemed to improve results.\n",
    "  * The deactivation state of such units is not very robust to noise, can get very negative.\n",
    "  * They suggest an activation function that can return negative values, but quickly saturates (for negative values, not for positive ones).\n",
    "  * So the model can make a quantitative assessment for positive statements (there is an amount X of A in the image), but only a qualitative negative one (something indicates that B is not in the image).\n",
    "  * They argue that this makes their activation function more robust to noise.\n",
    "  * Their activation function still has activations with a mean close to zero.\n",
    "\n",
    "* Zero Mean Activations Speed Up Learning\n",
    "  * Natural Gradient = Update direction which corrects the gradient direction with the Fisher Information Matrix\n",
    "  * Hessian-Free Optimization techniques use an extended Gauss-Newton approximation of Hessians and therefore can be interpreted as versions of natural gradient descent.\n",
    "  * Computing the Fisher matrix is too expensive for neural networks.\n",
    "  * Methods to approximate the Fisher matrix or to perform natural gradient descent have been developed.\n",
    "  * Natural gradient = inverse(FisherMatrix) * gradientOfWeights\n",
    "  * Lots of formulas. Apparently first explaining how the natural gradient descent works, then proofing that natural gradient descent can deal well with non-zero-mean activations.\n",
    "  * Natural gradient descent auto-corrects bias shift (i.e. non-zero-mean activations).\n",
    "  * If that auto-correction does not exist, oscillations (?) can occur, which slow down learning.\n",
    "  * Two ways to push means towards zero:\n",
    "    * Unit zero mean normalization (e.g. Batch Normalization)\n",
    "    * Activation functions with negative parts\n",
    "\n",
    "* Exponential Linear Units (ELUs)\n",
    "  * *Formula*\n",
    "    * f(x):\n",
    "      * if x >= 0: x\n",
    "      * else: alpha(exp(x)-1)\n",
    "    * f'(x) / Derivative:\n",
    "      * if x >= 0: 1\n",
    "      * else: f(x) + alpha\n",
    "    * `alpha` defines at which negative value the ELU saturates.\n",
    "    * `alpha=0.5` => minimum value is -0.5 (?)\n",
    "  * ELUs avoid the vanishing gradient problem, because their positive part is the identity function (like e.g. ReLUs)\n",
    "  * The negative values of ELUs push the mean activation towards zero.\n",
    "  * Mean activations closer to zero resemble more the natural gradient, therefore they should speed up learning.\n",
    "  * ELUs are more noise robust than PReLUs and LeakyReLUs, because their negative values saturate and thus should create a small gradient.\n",
    "  * \"ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence\"\n",
    "\n",
    "* Experiments Using ELUs\n",
    "  * They compare ELUs to ReLUs and LeakyReLUs, but not to PReLUs (no explanation why).\n",
    "  * They seem to use a negative slope of 0.1 for LeakyReLUs, even though 0.33 is standard afaik.\n",
    "  * They use an alpha of 1.0 for their ELUs (i.e. minimum value is -1.0).\n",
    "  * MNIST classification:\n",
    "    * ELUs achieved lower mean activations than ReLU/LeakyReLU\n",
    "    * ELUs achieved lower cross entropy loss than ReLU/LeakyReLU (and also seemed to learn faster)\n",
    "    * They used 5 hidden layers of 256 units each (no explanation why so many)\n",
    "    * (No convolutions)\n",
    "  * MNIST Autoencoder:\n",
    "    * ELUs performed consistently best (at different learning rates)\n",
    "    * Usually ELU > LeakyReLU > ReLU\n",
    "    * LeakyReLUs not far off, so if they had used a 0.33 value maybe these would have won\n",
    "  * CIFAR-100 classification:\n",
    "    * Convolutional network, 11 conv layers\n",
    "    * LeakyReLUs performed better during the first ~50 epochs, ReLUs mostly on par with ELUs\n",
    "    * LeakyReLUs about on par for epochs 50-100\n",
    "    * ELUs win in the end (the learning rates used might not be optimal for ELUs, were designed for ReLUs)\n",
    "  * CIFER-100, CIFAR-10 (big convnet):\n",
    "    * 6.55% error on CIFAR-10, 24.28% on CIFAR-100\n",
    "    * No comparison with ReLUs and LeakyReLUs for same architecture\n",
    "  * ImageNet\n",
    "    * Big convnet with spatial pyramid pooling (?) before the fully connected layers\n",
    "    * Network with ELUs performed better than ReLU network (better score at end, faster learning)\n",
    "    * Networks were still learning at the end, they didn't run till convergence\n",
    "    * No comparison to LeakyReLUs"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
