{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\n",
    "* **Authors**: Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer\n",
    "* **Link**: http://arxiv.org/abs/1602.07360v3\n",
    "* **Tags**: Neural Network, compression, AlexNet, residual\n",
    "* **Year**: 2016\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What\n",
    "  * The authors train a variant of AlexNet that has significantly fewer parameters than the original network, while keeping the network's accuracy stable.\n",
    "  * Advantages of this:\n",
    "    * More efficient distributed training, because less parameters have to be transferred.\n",
    "    * More efficient transfer via the internet, because the model's file size is smaller.\n",
    "    * Possibly less memory demand in production, because fewer parameters have to be kept in memory.\n",
    "\n",
    "* How\n",
    "  * They define a Fire Module. A Fire Module contains of:\n",
    "    * Squeeze Module: A 1x1 convolution that reduces the number of channels (e.g. from 128x32x32 to 64x32x32).\n",
    "    * Expand Module: A 1x1 convolution and a 3x3 convolution, both applied to the output of the Squeeze Module. Their results are concatenated.\n",
    "  * Using many 1x1 convolutions is advantageous, because they need less parameters than 3x3s.\n",
    "  * They use ReLUs, only convolutions (no fully connected layers) and Dropout (50%, before the last convolution).\n",
    "  * They use late maxpooling. They argue that applying pooling late - rather than early - improves accuracy while not needing more parameters.\n",
    "  * They try residual connections:\n",
    "    * One network without any residual connections (performed the worst).\n",
    "    * One network with residual connections based on identity functions, but only between layers of same dimensionality (performed the best).\n",
    "    * One network with residual connections based on identity functions and other residual connections with 1x1 convs (where dimensionality changed) (performance between the other two).\n",
    "  * They use pruning from Deep Compression to reduce the parameters further. Pruning simply collects the 50% of all parameters of a layer that have the lowest values and sets them to zero. That creates a sparse matrix.\n",
    "\n",
    "* Results\n",
    "  * 50x parameter reduction of AlexNet (1.2M parameters before pruning, 0.4M after pruning).\n",
    "  * 510x file size reduction of AlexNet (from 250mb to 0.47mb) when combined with Deep Compression.\n",
    "  * Top-1 accuracy remained stable.\n",
    "  * Pruning apparently can be used safely, even after the network parameters have already been reduced significantly.\n",
    "  * While pruning was generally safe, they found that two of their later layers reacted quite sensitive to it. Adding parameters to these (instead of removing them) actually significantly improved accuracy.\n",
    "  * Generally they found 1x1 convs to react more sensitive to pruning than 3x3s. Therefore they focused pruning on 3x3 convs.\n",
    "  * First pruning a network, then re-adding the pruned weights (initialized with 0s) and then retraining for some time significantly improved accuracy.\n",
    "  * The network was rather resilient to significant channel reduction in the Squeeze Modules. Reducing to 25-50% of the original channels (e.g. 128x32x32 to 64x32x32) seemed to be a good choice.\n",
    "  * The network was rather resilient to removing 3x3 convs and replacing them with 1x1 convs. A ratio of 2:1 to 1:1 (1x1 to 3x3) seemed to produce good results while mostly keeping the accuracy.\n",
    "  * Adding some residual connections between the Fire Modules improved the accuracy.\n",
    "  * Adding residual connections with identity functions *and also* residual connections with 1x1 convs (where dimensionality changed) improved the accuracy, but not as much as using *only* residual connections with identity functions (i.e. it's better to keep some modules without identity functions).\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "# Rough chapter-wise notes\n",
    "\n",
    "* (1) Introduction and Motivation\n",
    "  * Advantages from having less parameters:\n",
    "    * More efficient distributed training, because less data (parameters) have to be transfered.\n",
    "    * Less data to transfer to clients, e.g. when a model used by some app is updated.\n",
    "    * FPGAs often have hardly any memory, i.e. a model has to be small to be executed.\n",
    "  * Target here: Find a CNN architecture with less parameters than an existing one but comparable accuracy.\n",
    "\n",
    "* (2) Related Work\n",
    "  * (2.1) Model Compression\n",
    "    * SVD-method: Just apply SVD to the parameters of an existing model.\n",
    "    * Network Pruning: Replace parameters below threshold with zeros (-> sparse matrix), then retrain a bit.\n",
    "    * Add quantization and huffman encoding to network pruning = Deep Compression.\n",
    "  * (2.2) CNN Microarchitecture\n",
    "    * The term \"CNN Microarchitecture\" refers to the \"organization and dimensions of the individual modules\" (so an Inception module would have a complex CNN microarchitecture).\n",
    "  * (2.3) CNN Macroarchitecture\n",
    "    * CNN Macroarchitecture = \"big picture\" / organization of many modules in a network / general characteristics of the network, like depth\n",
    "    * Adding connections between modules can help (e.g. residual networks)\n",
    "  * (2.4) Neural Network Design Space Exploration\n",
    "    * Approaches for Design Space Exporation (DSE):\n",
    "      * Bayesian Optimization, Simulated Annealing, Randomized Search, Genetic Algorithms\n",
    "\n",
    "* (3) SqueezeNet: preserving accuracy with few parameters\n",
    "  * (3.1) Architectural Design Strategies\n",
    "    * A conv layer with N filters applied to CxHxW input (e.g. 3x128x128 for a possible first layer) with kernel size kHxkW (e.g. 3x3) has `N*C*kH*kW` parameters.\n",
    "    * So one way to reduce the parameters is to decrease kH and kW, e.g. from 3x3 to 1x1 (reduces parameters by a factor of 9).\n",
    "    * A second way is to reduce the number of channels (C), e.g. by using 1x1 convs before the 3x3 ones.\n",
    "    * They think that accuracy can be improved by performing downsampling later in the network (if parameter count is kept constant).\n",
    "  * (3.2) The Fire Module\n",
    "    * The Fire Module has two components:\n",
    "      * Squeeze Module:\n",
    "        * One layer of 1x1 convs\n",
    "      * Expand Module:\n",
    "        * Concat the results of:\n",
    "          * One layer of 1x1 convs\n",
    "          * One layer of 3x3 convs\n",
    "    * The Squeeze Module decreases the number of input channels significantly.\n",
    "    * The Expand Module then increases the number of input channels again.\n",
    "  * (3.3) The SqueezeNet architecture\n",
    "    * One standalone conv, then several fire modules, then a standalone conv, then global average pooling, then softmax.\n",
    "    * Three late max pooling laters.\n",
    "    * Gradual increase of filter numbers.\n",
    "    * (3.3.1) Other SqueezeNet details\n",
    "      * ReLU activations\n",
    "      * Dropout before the last conv layer.\n",
    "      * No linear layers.\n",
    "\n",
    "* (4) Evaluation of SqueezeNet\n",
    "  * Results of competing methods:\n",
    "    * SVD: 5x compression, 56% top-1 accuracy\n",
    "    * Pruning: 9x compression, 57.2% top-1 accuracy\n",
    "    * Deep Compression: 35x compression, ~57% top-1 accuracy\n",
    "  * SqueezeNet: 50x compression, ~57% top-1 accuracy\n",
    "  * SqueezeNet combines low parameter counts with Deep Compression.\n",
    "  * The accuracy does not go down because of that, i.e. apparently Deep Compression can even be applied to small models without giving up on performance.\n",
    "\n",
    "* (5) CNN Microarchitecture Design Space Exploration\n",
    "  * (5.1) CNN Microarchitecture metaparameters\n",
    "    * blabla we test various values for this and that parameter\n",
    "  * (5.2) Squeeze Ratio\n",
    "    * In a Fire Module there is first a Squeeze Module and then an Expand Module. The Squeeze Module decreases the number of input channels to which 1x1 and 3x3 both are applied (at the same time).\n",
    "    * They analyzed how far you can go down with the Sqeeze Module by training multiple networks and calculating the top-5 accuracy for each of them.\n",
    "    * The accuracy by Squeeze Ratio (percentage of input channels kept in 1x1 squeeze, i.e. 50% = reduced by half, e.g. from 128 to 64):\n",
    "      * 12%: ~80% top-5 accuracy\n",
    "      * 25%: ~82% top-5 accuracy\n",
    "      * 50%: ~85% top-5 accuracy\n",
    "      * 75%: ~86% top-5 accuracy\n",
    "      * 100%: ~86% top-5 accuracy\n",
    "  * (5.3) Trading off 1x1 and 3x3 filters\n",
    "    * Similar to the Squeeze Ratio, they analyze the optimal ratio of 1x1 filters to 3x3 filters.\n",
    "    * E.g. 50% would mean that half of all filters in each Fire Module are 1x1 filters.\n",
    "    * Results:\n",
    "      * 01%: ~76% top-5 accuracy\n",
    "      * 12%: ~80% top-5 accuracy\n",
    "      * 25%: ~82% top-5 accuracy\n",
    "      * 50%: ~85% top-5 accuracy\n",
    "      * 75%: ~85% top-5 accuracy\n",
    "      * 99%: ~85% top-5 accuracy\n",
    "\n",
    "* (6) CNN Macroarchitecture Design Space Exploration\n",
    "  * They compare the following networks:\n",
    "    * (1) Without residual connections\n",
    "    * (2) With residual connections between modules of same dimensionality\n",
    "    * (3) With residual connections between all modules (except pooling layers) using 1x1 convs (instead of identity functions) where needed\n",
    "  * Adding residual connections (2) improved top-1 accuracy from 57.5% to 60.4% without any new parameters.\n",
    "  * Adding complex residual connections (3) worsed top-1 accuracy again to 58.8%, while adding new parameters.\n",
    "\n",
    "* (7) Model Compression Design Space Exploration\n",
    "  * (7.1) Sensitivity Analysis: Where to Prune or Add parameters\n",
    "    * They went through all layers (including each one in the Fire Modules).\n",
    "    * In each layer they set the 50% smallest weights to zero (pruning) and measured the effect on the top-5 accuracy.\n",
    "    * It turns out that doing that has basically no influence on the top-5 accuracy in most layers.\n",
    "    * Two layers towards the end however had significant influence (accuracy went down by 5-10%).\n",
    "    * Adding parameters to these layers improved top-1 accuracy from 57.5% to 59.5%.\n",
    "    * Generally they found 1x1 layers to be more sensitive than 3x3 layers so they pruned them less aggressively.\n",
    "  * (7.2) Improving Accuracy by Densifying Sparse Models\n",
    "    * They found that first pruning a model and then retraining it again (initializing the pruned weights to 0) leads to higher accuracy.\n",
    "    * They could improve top-1 accuracy by 4.3% in this way."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
