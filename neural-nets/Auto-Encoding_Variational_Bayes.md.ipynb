{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: Auto-Encoding Variational Bayes\n",
    "* **Authors**: Diederik P Kingma, Max Welling\n",
    "* **Link**: http://arxiv.org/abs/1312.6114\n",
    "* **Tags**: Neural Network, Autoencoder, VAE\n",
    "* **Year**: 2014\n",
    "\n",
    "# Summary\n",
    "\n",
    "![Architecture](images/DenseCap__architecture.png?raw=true \"Architecture.\")\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "# Rough chapter-wise notes\n",
    "\n",
    "* (1) Introduction\n",
    "  * In a directed probabilistic model we latent/hidden variables with intractable posterior distributions.\n",
    "  * Variational Bayesian Approach (VB): Optimize an approximation to these posteriors.\n",
    "  * Common VB method: mean-field, which (in the general case) also has intractable posteriors.\n",
    "  * Their SGVB estimator (Stochastic Gradient Variational Bayes):\n",
    "    * Is an unbiased estimator of the variational lower bound.\n",
    "    * Is based on a reparameterization of the variational lower bound.\n",
    "    * Is differentiable. Can be optimized with SGD.\n",
    "    * Can be used for approximate posterior inference (in almost any model, must(?) have continuous latent variables/parameters).\n",
    "  * AEVB algorithm (Auto-Encoding Variational Bayes):\n",
    "    * Works with iid datasets that have continuous latent variables.\n",
    "    * Optimizes a recognition model (the approximation to the posterior?).\n",
    "    * That optimization is done using the SGVB estimator.\n",
    "    * The recognition model can be used for efficient learning and approximate inference using ancestral sampling (apparently another name for forward sampling?).\n",
    "    * If the recognition model is a neural net, it is called \"variational auto-encoder\".\n",
    "\n",
    "* (2) Method\n",
    "  * Assumptions:\n",
    "    * iid dataset.\n",
    "    * Continuous latent variables per datapoint.\n",
    "    * Inference via MLE or MAP on the parameters.\n",
    "    * Variational inference on the latent variables.\n",
    "  * Strategy: Derive lower bound estimator (for a variety of directed graphical models with continuous latent variables).\n",
    "  * (2.1) Problem scenario\n",
    "    * Assume we have a dataset.\n",
    "    * Assume that each entry in that dataset has one observed variable `x` and one unobserved variable `z`.\n",
    "    * Assume that `z` has no parents and just follows the prior `p(z)`.\n",
    "    * Assume that `x` has `z` as its parent and follows the likelihood `p(x|z)`.\n",
    "    * Assume that `p(z)` and `p(x|z)` come from parametric families of distributions (e.g. normal distribution), which have the parameters `theta`.\n",
    "    * Assume that the probability density functions (PDFs) of `p(z)` and `p(x|z)` are both differentiable (almost) everywhere (with respect to `theta` and `z`).\n",
    "    * No simpifying assumption for the marginal probabilities or conditional probabilities.\n",
    "    * The algorithm is supposed to work even when (1) p(x) and/or p(z|x) are intractable with standard VB methods and (2) when we have huge datasets that require processing in minibatches.\n",
    "    * Interesting tasks to solve:\n",
    "      * (Efficient) MLE and MAP estimation of parameters `theta`.\n",
    "      * Inference of p(z|x). Examples: Dimensionality reduction.\n",
    "      * Inference of p(x). Examples: Denoising, inpainting, super-resolution.\n",
    "    * They introduce a recognition model q(z|x) that approximates p(z|x) (e.g. from an image to latent variables like rotation, scaling, ...).\n",
    "    * q(z|x) can be viewed as an encoder that takes an x and produces a distribution (e.g. Gaussian) over z.\n",
    "    * p(x|z) can be viewed as a decoder that takes a z and produces a distribution (e.g. Gaussian) over x.\n",
    "    * z can be viewed as the code layer and any configuration of z as a code.\n",
    "  * (2.2) The variational bound\n",
    "    * The probability of the whole dataset is `p(X) = p(x1)*p(x2)*...`.\n",
    "    * Or with log: `log p(x) = log p(x1) + log p(x2) + ...`.\n",
    "    * The probability of a single datapoint is `log p(xi) = KL(q(z|xi) || p(z||xi)) + L(theta, phi; xi)` (no real explanation where this suddenly comes from).\n",
    "    * KL(a||b) is the KL-divergence between p(z|x) and p(z|x)'s approximation, i.e. q(z|x). The KL-Divergence measures the difference between the two probability distributions. It is always `>=0`. It is close to 0 if the approximation is very close to the real distribution.\n",
    "    * `L(theta, phi; xi)` is the variational lower bound. It is defined as `-KL(q(z|xi) || p(z)) + E[log p(xi|z)]`. `theta` are the parameters of p(x|z) and p(z). `phi` are the paramters of p(z|x).\n",
    "    * We want to find optimal parameters for `theta` and `phi`. Usually one would use Monte Carlo methods for that, but they lead to too much variance.\n",
    "  * (2.3) The SGVB estimator and AEVB algorithm\n",
    "    * This section discusses a method to estimate the lower bound and its derivatives wrt to the parameters.\n",
    "    * Instead of using the probability distribution q(z|x) (e.g. from an image to the latent variables) we use the estimator z=g(epsilon, x) where epsilon is a noise variable (e.g. from an image and some noise to the latent variables).\n",
    "    * Using Monte Carlo Methods, one can now approximate the variational lower bound: `La(theta, phi; xi) = 1/L sum log p(xi, g(epsilon, xi)) - log q(g(epsilon, xi)|xi)`\n",
    "    * The given formula is for the full lower bound, i.e. for `L = -KL(q(z|xi)||p(z)) + E[log p(xi|z)] = -ComplexitPenalty + ReconstructionError`. The KL-Divergence can usually be computed analytically, so sampling it would be wasteful. That leads to the formular `Lb = -KL(q(z|xi)||p(z)) + Sample[log p(xi|zi)] = -KL(q(z|xi)||p(z)) + 1/L sum (log p(xi|zi))`.\n",
    "    * For a dataset with N examples and a batch size of M, the lower bound can be approximated via `N/M sum L(theta, phi; xi)`.\n",
    "    * The number of required samples L can be reduced to one if the batch size is high enough (>= 100).\n",
    "    * Stochastic Gradient Descent can be used to approximate the lower bound.\n",
    "    * The lower bound approximation is reminiscent of auto-encoders, which also usually have an objective function of the form `ReconstructionError - ComplexityPenalty`, i.e. here `E[log p(xi|z)] - KLDivergence`.\n",
    "    * The basic principle of a VAE is: Take an input example xi, then generate some noise, then use g(xi, noise) to generate a sample zi of the latent variables, then take zi and convert it via p(xi|zi) back to the input, resulting in some reconstruction error.\n",
    "  * (2.4) The reparameterization trick\n",
    "    *"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
