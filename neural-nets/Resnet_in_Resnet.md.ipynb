{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: Resnet in Resnet: Generalizing Residual Architectures\n",
    "* **Authors**: Sasha Targ, Diogo Almeida, Kevin Lyman\n",
    "* **Link**: http://arxiv.org/abs/1603.08029\n",
    "* **Tags**: Neural Network, residual\n",
    "* **Year**: 2016\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What\n",
    "  * They describe an architecture that merges classical convolutional networks and residual networks.\n",
    "  * The architecture can (theoretically) learn anything that a classical convolutional network or a residual network can learn, as it contains both of them.\n",
    "  * The architecture can (theoretically) learn how many convolutional layers it should use per residual block (up to the amount of convolutional layers in the whole network).\n",
    "\n",
    "* How\n",
    "  * Just like residual networks, they have \"blocks\". Each block contains convolutional layers.\n",
    "  * Each block contains residual units and non-residual units.\n",
    "  * They have two \"streams\" of data in their network (just matrices generated by each block):\n",
    "    * Residual stream: The residual blocks write to this stream (i.e. it's their output).\n",
    "    * Transient stream: The non-residual blocks write to this stream.\n",
    "  * Residual and non-residual layers receive *both* streams as input, but only write to *their* stream as output.\n",
    "  * Their architecture visualized:\n",
    "    ![Architecture](images/Resnet_in_Resnet__architecture.png?raw=true \"Architecture\")\n",
    "  * Because of this architecture, their model can learn the number of layers per residual block (though BN and ReLU might cause problems here?):\n",
    "    ![Learning layercount](images/Resnet_in_Resnet__learning_layercount.png?raw=true \"Learning layercount\")\n",
    "  * The easiest way to implement this should be along the lines of the following (some of the visualized convolutions can be merged):\n",
    "    * Input of size CxHxW (both streams, each C/2 planes)\n",
    "      * Concat\n",
    "        * Residual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards.\n",
    "        * Transient block: Apply C/2 convolutions to the C input planes.\n",
    "      * Apply BN\n",
    "      * Apply ReLU\n",
    "    * Output of size CxHxW.\n",
    "  * The whole operation can also be implemented with just a single convolutional layer, but then one has to make sure that some weights stay at zero.\n",
    "\n",
    "* Results\n",
    "  * They test on CIFAR-10 and CIFAR-100.\n",
    "  * They search for optimal hyperparameters (learning rate, optimizer, L2 penalty, initialization method, type of shortcut connection in residual blocks) using a grid search.\n",
    "  * Their model improves upon a wide ResNet and an equivalent non-residual CNN by a good margin (CIFAR-10: 0.5-1%, CIFAR-100: 1-2%)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
