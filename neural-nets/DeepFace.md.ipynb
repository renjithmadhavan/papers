{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: DeepFace: Closing the Gap to Human-Level Performance in Face Verification\n",
    "* **Authors**: Yaniv Taigman, Ming Yang, Marcâ€™Aurelio Ranzato, Lior Wolf\n",
    "* **Link**: https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf\n",
    "* **Tags**: Neural Network, face\n",
    "* **Year**: 2014\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What\n",
    "  * They describe a CNN architecture that can be used to identify a person given an image of their face.\n",
    "\n",
    "* How\n",
    "  * The expected input is the image of a face (i.e. it does not search for faces in images, the faces already have to be extracted by a different method).\n",
    "  * *Face alignment / Frontalization*\n",
    "    * Target of this step: Get rid of variations within the face images, so that every face seems to look straight into the camera (\"frontalized\").\n",
    "    * 2D alignment\n",
    "      * They search for landmarks (fiducial points) on the face.\n",
    "        * They use SVRs (features: LBPs) for that.\n",
    "        * After every application of the SVR, the localized landmarks are used to transform/normalize the face. Then the SVR is applied again. By doing this, the locations of the landmarks are gradually refined.\n",
    "      * They use the detected landmarks to normalize the face images (via scaling, rotation and translation).\n",
    "    * 3D alignment\n",
    "      * The 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side). To normalize out-of-plane variations they need a 3D transformation.\n",
    "      * They detect an additional 67 landmarks on the faces (again via SVRs).\n",
    "      * They construct a human face mesh from a dataset (USF Human-ID).\n",
    "      * They map the 67 landmarks to that mesh.\n",
    "      * They then use some more complicated steps to recover the frontalized face image.\n",
    "  * *CNN architecture*\n",
    "    * The CNN receives the frontalized face images (152x152, RGB).\n",
    "    * It then applies the following steps:\n",
    "      * Convolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW)\n",
    "      * Max pooling over 3x3, stride 2 (-> 32x71x71)\n",
    "      * Convolution, 16 filters, 9x9, ReLU (-> 16x63x63)\n",
    "      * Local Convolution, 16 filters, 9x9, ReLU (-> 16x55x55)\n",
    "      * Local Convolution, 16 filters, 7x7, ReLU (-> 16x25x25)\n",
    "      * Local Convolution, 16 filters, 5x5, ReLU (-> 16x21x21)\n",
    "      * Fully Connected, 4096, ReLU\n",
    "      * Fully Connected, 4030, Softmax\n",
    "    * Local Convolutions use a different set of learned weights at every \"pixel\" (while a normal convolution uses the same set of weights at all locations).\n",
    "    * They can afford to use local convolutions because of their frontalization, which roughly forces specific landmarks to be at specific locations.\n",
    "    * They use dropout (apparently only after the first fully connected layer).\n",
    "    * They normalize \"the features\" (probably the 4096 fully connected layer). Each component is divided by its maximum value across a training set. Additionally, the whole vector is L2-normalized. The goal of this step is to make the network less sensitive to illumination changes.\n",
    "    * The whole network has about 120 million parameters.\n",
    "    * Visualization of the architecture:\n",
    "      * ![Architecture](images/DeepFace__architecture.jpg?raw=true \"Architecture\")\n",
    "  * *Training*\n",
    "    * The network receives images, each showing a face, and is trained to classify the identity of the face (e.g. gets image of Obama, has to return \"that's Obama\").\n",
    "    * They use cross-entropy as their loss.\n",
    "  * *Face verification*\n",
    "    * In order to tell whether two images of faces show the same person they try three different methods.\n",
    "    * Each of these relies on the vector extracted by the first fully connected layer in the network (4096d).\n",
    "    * Let these vectors be `f1` (image 1) and `f2` (image 2). The methods are then:\n",
    "      1. Inner product between `f1` and `f2`. The classification (same person/not same person) is then done by a simple threshold.\n",
    "      2. Weighted X^2 (chi-squared) distance. Equation, per vector component i: `weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i])`. The vector is then fed into an SVM.\n",
    "      3. Siamese network. Means here simply that the absolute distance between `f1` and `f2` is calculated (`|f1-f2|`), each component is weighted by a learned weight and then the sum of the components is calculated. If the result is above a threshold, the faces are considered to show the same person.\n",
    "\n",
    "* Results\n",
    "  * They train their network on the Social Face Classification (SFC) dataset. That seems to be a Facebook-internal dataset (i.e. not public) with 4.4 million faces of 4k people.\n",
    "  * When applied to the LFW dataset:\n",
    "    * Face recognition (\"which person is shown in the image\") (apparently they retrained the whole model on LFW for this task?):\n",
    "      * Simple SVM with LBP (i.e. not their network): 91.4% mean accuracy.\n",
    "      * Their model, with frontalization, with 2d alignment: ??? no value.\n",
    "      * Their model, no frontalization (only 2d alignment): 94.3% mean accuracy.\n",
    "      * Their model, no frontalization, no 2d alignment: 87.9% mean accuracy.\n",
    "    * Face verification (two images -> same/not same person) (apparently also trained on LFW? unclear):\n",
    "      * Method 1 (inner product + threshold): 95.92% mean accuracy.\n",
    "      * Method 2 (X^2 vector + SVM): 97.00% mean accurracy.\n",
    "      * Method 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset).\n",
    "  * When applied to the YTF dataset (YouTube video frames):\n",
    "    * 92.5% accuracy via X^2-method."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
