{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: DRAW: A Recurrent Neural Network For Image Generation\n",
    "* **Authors**: Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra\n",
    "* **Link**: http://arxiv.org/abs/1502.04623\n",
    "* **Tags**: Neural Network, generative models, recurrent, attention\n",
    "* **Year**: 2015\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What\n",
    "  * DRAW = deep recurrent attentive writer\n",
    "  * DRAW is a recurrent autoencoder for (primarily) images that uses attention mechanisms.\n",
    "  * Like all autoencoders it has an encoder, a latent layer `Z` in the \"middle\" and a decoder.\n",
    "  * Due to the recurrence, there are actually multiple autoencoders, one for each timestep (the number of timesteps is fixed).\n",
    "  * DRAW has attention mechanisms which allow the model to decide where to look at in the input image (\"glimpses\") and where to write/draw to in the output image.\n",
    "  * If the attention mechanisms are skipped, the model becomes a simple recurrent autoencoder.\n",
    "  * By training the full autoencoder on a dataset and then only using the decoder, one can generate new images that look similar to the dataset images.\n",
    "\n",
    "\n",
    "![DRAW Architecture](images/DRAW_A_Recurrent_Neural_Network_for_Image_Generation__architecture.png?raw=true \"DRAW Architecture\")\n",
    "\n",
    "*Basic recurrent architecture of DRAW.*\n",
    "\n",
    "\n",
    "* How\n",
    "  * General architecture\n",
    "    * The encoder-decoder-pair follows the design of variational autoencoders.\n",
    "    * The latent layer follows an n-dimensional gaussian distribution. The hyperparameters of that distribution (means, standard deviations) are derived from the output of the encoder using a linear transformation.\n",
    "    * Using a gaussian distribution enables the use of the reparameterization trick, which can be useful for backpropagation.\n",
    "    * The decoder receives a sample drawn from that gaussian distribution.\n",
    "    * While the encoder reads from the input image, the decoder writes to an image canvas (where \"write\" is an addition, not a replacement of the old values).\n",
    "    * The model works in a fixed number of timesteps. At each timestep the encoder performs a read operation and the decoder a write operation.\n",
    "    * Both the encoder and the decoder receive the previous output of the encoder.\n",
    "  * Loss functions\n",
    "    * The loss function of the latent layer is the KL-divergence between that layer's gaussian distribution and a prior, summed over the timesteps.\n",
    "    * The loss function of the decoder is the negative log likelihood of the image given the final canvas content under a bernoulli distribution.\n",
    "    * The total loss, which is optimized, is the expectation of the sum of both losses (latent layer loss, decoder loss).\n",
    "  * Attention\n",
    "    * The selective read attention works on image patches of varying sizes. The result size is always NxN.\n",
    "    * The mechanism has the following parameters:\n",
    "        * `gx`: x-axis coordinate of the center of the patch\n",
    "        * `gy`: y-axis coordinate of the center of the patch\n",
    "        * `delta`: Strides. The higher the strides value, the larger the read image patch.\n",
    "        * `sigma`: Standard deviation. The higher the sigma value, the more blurry the extracted patch will be.\n",
    "        * `gamma`: Intensity-Multiplier. Will be used on the result.\n",
    "        * All of these parameters are generated using a linear transformation applied to the decoder's output.\n",
    "    * The mechanism places a grid of NxN gaussians on the image. The grid is centered at `(gx, gy)`. The gaussians are `delta` pixels apart from each other and have a standard deviation of `sigma`.\n",
    "    * Each gaussian is applied to the image, the center pixel is read and added to the result.\n",
    "\n",
    "\n",
    "![DRAW Attention](images/DRAW_A_Recurrent_Neural_Network_for_Image_Generation__attention.png?raw=true \"DRAW Attention\")\n",
    "\n",
    "*The basic attention mechanism. (gx, gy) is the read patch center. delta is the strides. On the right: Patches with different sizes/strides and standard deviations/blurriness.*\n",
    "\n",
    "\n",
    "* Results\n",
    "  * Realistic looking generated images for MNIST and SVHN.\n",
    "  * Structurally OK, but overall blurry images for CIFAR-10.\n",
    "  * Results with attention are usually significantly better than without attention.\n",
    "  * Image generation without attention starts with a blurry image and progressively sharpens it.\n",
    "\n",
    "![DRAW SVHN Results](images/DRAW_A_Recurrent_Neural_Network_for_Image_Generation__svhn.png?raw=true \"DRAW SVHN Results\")\n",
    "\n",
    "*Using DRAW with attention to generate new SVHN images.*\n",
    "\n",
    "----------\n",
    "\n",
    "# Rough chapter-wise notes\n",
    "\n",
    "* 1. Introduction\n",
    "  * The natural way to draw an image is in a step by step way (add some lines, then add some more, etc.).\n",
    "  * Most generative neural networks however create the image in one step.\n",
    "  * That removes the possibility of iterative self-correction, is hard to scale to large images and makes the image generation process dependent on a single latent distribution (input parameters).\n",
    "  * The DRAW architecture generates images in multiple steps, allowing refinements/corrections.\n",
    "  * DRAW is based on varational autoencoders: An encoder compresses images to codes and a decoder generates images from codes.\n",
    "  * The loss function is a variational upper bound on the log-likelihood of the data.\n",
    "  * DRAW uses recurrance to generate images step by step.\n",
    "  * The recurrance is combined with attention via partial glimpses/foveations (i.e. the model sees only a small part of the image).\n",
    "  * Attention is implemented in a differentiable way in DRAW.\n",
    "\n",
    "* 2. The DRAW Network\n",
    "  * The DRAW architecture is based on variational autoencoders:\n",
    "    * Encoder: Compresses an image to latent codes, which represent the information contained in the image.\n",
    "    * Decoder: Transforms the codes from the encoder to images (i.e. defines a distribution over images which is conditioned on the distribution of codes).\n",
    "  * Differences to variational autoencoders:\n",
    "    * Encoder and decoder are both recurrent neural networks.\n",
    "    * The encoder receives the previous output of the decoder.\n",
    "    * The decoder writes several times to the image array (instead of only once).\n",
    "    * The encoder has an attention mechanism. It can make a decision about the read location in the input image.\n",
    "    * The decoder has an attention mechanism. It can make a decision about the write location in the output image.\n",
    "  * 2.1 Network architecture\n",
    "    * They use LSTMs for the encoder and decoder.\n",
    "    * The encoder generates a vector.\n",
    "    * The decoder generates a vector.\n",
    "    * The encoder receives at each time step the image and the output of the previous decoding step.\n",
    "    * The hidden layer in between encoder and decoder is a distribution Q(Zt|ht^enc), which is a diagonal gaussian.\n",
    "    * The mean and standard deviation of that gaussian is derived from the encoder's output vector with a linear transformation.\n",
    "    * Using a gaussian instead of a bernoulli distribution enables the use of the reparameterization trick. That trick makes it straightforward to backpropagate \"low variance stochastic gradients of the loss function through the latent distribution\".\n",
    "    * The decoder writes to an image canvas. At every timestep the vector generated by the decoder is added to that canvas.\n",
    "  * 2.2 Loss function\n",
    "    * The main loss function is the negative log probability: `-log D(x|ct)`, where `x` is the input image and `ct` is the final output image of the autoencoder. `D` is a bernoulli distribution if the image is binary (only 0s and 1s).\n",
    "    * The model also uses a latent loss for the latent layer (between encoder and decoder). That is typical for VAEs. The loss is the KL-Divergence between Q(Zt|ht_enc) (`Zt` = latent layer, `ht_enc` = result of encoder) and a prior `P(Zt)`.\n",
    "    * The full loss function is the expection value of both losses added up.\n",
    "  * 2.3 Stochastic Data Generation\n",
    "    * To generate images, samples can be picked from the latent layer based on a prior. These samples are then fed into the decoder. That is repeated for several timesteps until the image is finished.\n",
    "\n",
    "* 3. Read and Write Operations\n",
    "  * 3.1 Reading and writing without attention\n",
    "    * Without attention, DRAW simply reads in the whole image and modifies the whole output image canvas at every timestep.\n",
    "  * 3.2 Selective attention model\n",
    "    * The model can decide which parts of the image to read, i.e. where to look at. These looks are called glimpses.\n",
    "    * Each glimpse is defined by its center (x, y), its stride (zoom level), its gaussian variance (the higher the variance, the more blurry is the result) and a scalar multiplier (that scales the intensity of the glimpse result).\n",
    "    * These parameters are calculated based on the decoder output using a linear transformation.\n",
    "    * For an NxN patch/glimpse `N*N` gaussians are created and applied to the image. The center pixel of each gaussian is then used as the respective output pixel of the glimpse.\n",
    "  * 3.3 Reading and writing with attention\n",
    "    * Mostly the same technique from (3.2) is applied to both reading and writing.\n",
    "    * The glimpse parameters are generated from the decoder output in both cases. The parameters can be different (i.e. read and write at different positions).\n",
    "    * For RGB the same glimpses are applied to each channel.\n",
    "\n",
    "* 4. Experimental results\n",
    "  * They train on binary MNIST, cluttered MNIST, SVHN and CIFAR-10.\n",
    "  * They then classfiy the images (cluttered MNIST) or generate new images (other datasets).\n",
    "  * They say that these generated images are unique (to which degree?) and that they look realistic for MNIST and SVHN.\n",
    "  * Results on CIFAR-10 are blurry.\n",
    "  * They use binary crossentropy as the loss function for binary MNIST.\n",
    "  * They use crossentropy as the loss function for SVHN and CIFAR-10 (color).\n",
    "  * They used Adam as their optimizer.\n",
    "  * 4.1 Cluttered MNIST classification\n",
    "    * They classify images of cluttered MNIST. To do that, they use an LSTM that performs N read-glimpses and then classifies via a softmax layer.\n",
    "    * Their model's error rate is significantly below a previous non-differentiable attention based model.\n",
    "    * Performing more glimpses seems to decrease the error rate further.\n",
    "  * 4.2 MNIST generation\n",
    "    * They generate binary MNIST images using only the decoder.\n",
    "    * DRAW without attention seems to perform similarly to previous models.\n",
    "    * DRAW with attention seems to perform significantly better than previous models.\n",
    "    * DRAW without attention progressively sharpens images.\n",
    "    * DRAW with attention draws lines by tracing them.\n",
    "  * 4.3 MNIST generation with two digits\n",
    "    * They created a dataset of 60x60 images, each of them containing two random 28x28 MNIST images.\n",
    "    * They then generated new images using only the decoder.\n",
    "    * DRAW learned to do that.\n",
    "    * Using attention, the model usually first drew one digit then the other.\n",
    "  * 4.4 Street view house number generation\n",
    "    * They generate SVHN images using only the decoder.\n",
    "    * Results look quite realistic.\n",
    "  * 4.5 Generating CIFAR images\n",
    "    * They generate CIFAR-10 images using only the decoder.\n",
    "    * Results follow roughly the structure of CIFAR-images, but look blurry."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
