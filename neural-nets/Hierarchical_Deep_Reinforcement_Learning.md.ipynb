{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "\n",
    "* **Title**: Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\n",
    "* **Authors**: Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum\n",
    "* **Link**: https://arxiv.org/abs/1604.06057\n",
    "* **Tags**: Neural Network, reinforcement learning\n",
    "* **Year**: 2016\n",
    "\n",
    "# Summary\n",
    "\n",
    "* What\n",
    "  * They present a hierarchical method for reinforcement learning.\n",
    "  * The method combines \"long\"-term goals with short-term action choices.\n",
    "\n",
    "* How\n",
    "  * They have two components:\n",
    "    * Meta-Controller:\n",
    "      * Responsible for the \"long\"-term goals.\n",
    "      * Is trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions.\n",
    "      * The Meta-Controller only picks goals when the Controller terminates or achieved the goal.\n",
    "    * Controller:\n",
    "      * Receives the current state and the current goal.\n",
    "      * Has to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here).\n",
    "      * The reward is intrinsic. It comes from the Critic. The Critic gives reward whenever the current goal is reached.\n",
    "  * For Montezuma's Revenge:\n",
    "    * A goal is to reach a specific object.\n",
    "    * The goal is encoded via a bitmask (as big as the game screen). The mask contains 1s wherever the object is.\n",
    "    * They hand-extract the location of a few specific objects.\n",
    "    * So basically:\n",
    "      * The Meta-Controller picks the next object to reach via a Q-value function.\n",
    "      * It receives extrinsic reward when objects have been reached in a specific sequence.\n",
    "      * The Controller picks actions that lead to reaching the object based on a Q-value function. It iterates action-choosing until it terminates or reached the goal-object.\n",
    "      * The Critic awards intrinsic reward to the Controller whenever the goal-object was reached.\n",
    "    * They use CNNs for the Meta-Controller and the Controller, similar in architecture to the Atari-DQN paper (shallow CNNs).\n",
    "    * They use two replay memories, one for the Meta-Controller (size 40k) and one for the Controller (size 1M).\n",
    "    * Both follow an epsilon-greedy policy (for picking goals/actions). Epsilon starts at 1.0 and is annealed down to 0.1.\n",
    "    * They use a discount factor / gamma of 0.9.\n",
    "    * They train with SGD. \n",
    "\n",
    "* Results\n",
    "  * Learns to play Montezuma's Revenge.\n",
    "  * Learns to act well in a more abstract MDP with delayed rewards and where simple Q-learning failed.\n",
    "\n",
    "--------------------\n",
    "\n",
    "# Rough chapter-wise notes\n",
    "\n",
    "\n",
    "* (1) Introduction\n",
    "  * Basic problem: Learn goal directed behaviour from sparse feedbacks.\n",
    "  * Challenges:\n",
    "    * Explore state space efficiently\n",
    "    * Create multiple levels of spatio-temporal abstractions\n",
    "  * Their method: Combines deep reinforcement learning with hierarchical value functions.\n",
    "  * Their agent is motivated to solve specific intrinsic goals.\n",
    "  * Goals are defined in the space of entities and relations, which constraints the search space.\n",
    "  * They define their value function as V(s, g) where s is the state and g is a goal.\n",
    "  * First, their agent learns to solve intrinsically generated goals. Then it learns to chain these goals together.\n",
    "  * Their model has two hiearchy levels:\n",
    "    * Meta-Controller: Selects the current goal based on the current state.\n",
    "    * Controller: Takes state s and goal g, then selects a good action based on s and g. The controller operates until g is achieved, then the meta-controller picks the next goal.\n",
    "  * Meta-Controller gets extrinsic rewards, controller gets intrinsic rewards.\n",
    "  * They use SGD to optimize the whole system (with respect to reward maximization).\n",
    "\n",
    "* (3) Model\n",
    "  * Basic setting: Action a out of all actions A, state s out of S, transition function T(s,a)->s', reward by state F(s)->R.\n",
    "  * epsilon-greedy is good for local exploration, but it's not good at exploring very different areas of the state space.\n",
    "  * They use intrinsically motivated goals to better explore the state space.\n",
    "  * Sequences of goals are arranged to maximize the received extrinsic reward.\n",
    "  * The agent learns one policy per goal.\n",
    "  * Meta-Controller: Receives current state, chooses goal.\n",
    "  * Controller: Receives current state and current goal, chooses action. Keeps choosing actions until goal is achieved or a terminal state is reached. Has the optimization target of maximizing cumulative reward.\n",
    "  * Critic: Checks if current goal is achieved and if so provides intrinsic reward.\n",
    "  * They use deep Q learning to train their model.\n",
    "  * There are two Q-value functions. One for the controller and one for the meta-controller.\n",
    "  * Both formulas are extended by the last chosen goal g.\n",
    "  * The Q-value function of the meta-controller does not depend on the chosen action.\n",
    "  * The Q-value function of the controller receives only intrinsic direct reward, not extrinsic direct reward.\n",
    "  * Both Q-value functions are reprsented with DQNs.\n",
    "  * Both are optimized to minimize MSE losses.\n",
    "  * They use separate replay memories for the controller and meta-controller.\n",
    "  * A memory is added for the meta-controller whenever the controller terminates.\n",
    "  * Each new goal is picked by the meta-controller epsilon-greedy (based on the current state).\n",
    "  * The controller picks actions epsilon-greedy (based on the current state and goal).\n",
    "  * Both epsilons are annealed down.\n",
    "\n",
    "* (4) Experiments\n",
    "  * (4.1) Discrete MDP with delayed rewards\n",
    "    * Basic MDP setting, following roughly: Several states (s1 to s6) organized in a chain. The agent can move left or right. It gets high reward if it moves to state s6 and then back to s1, otherwise it gets small reward per reached state.\n",
    "    * They use their hierarchical method, but without neural nets.\n",
    "    * Baseline is Q-learning without a hierarchy/intrinsic rewards.\n",
    "    * Their method performs significantly better than the baseline.\n",
    "  * (4.2) ATARI game with delayed rewards\n",
    "    * They play Montezuma's Revenge with their method, because that game has very delayed rewards.\n",
    "    * They use CNNs for the controller and meta-controller (architecture similar to the Atari-DQN paper).\n",
    "    * The critic reacts to (entity1, relation, entity2) relationships. The entities are just objects visible in the game. The relation is (apparently ?) always \"reached\", i.e. whether object1 arrived at object2.\n",
    "    * They extract the objects manually, i.e. assume the existance of a perfect unsupervised object detector.\n",
    "    * They encode the goals apparently not as vectors, but instead just use a bitmask (game screen heightand width), which has 1s at the pixels that show the object.\n",
    "    * Replay memory sizes: 1M for controller, 50k for meta-controller.\n",
    "    * gamma=0.99\n",
    "    * They first only train the controller (i.e. meta-controller completely random) and only then train both jointly.\n",
    "    * Their method successfully learns to perform actions which lead to rewards with long delays.\n",
    "    * It starts with easier goals and then learns harder goals."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
